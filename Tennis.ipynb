{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3: Collaboration and competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we start and interact with the environment to train agents which play tennis collaboratively.\n",
    "\n",
    "### 1. Necessary imports and defining the constants\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from unityagents import UnityEnvironment\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from d4pg_agent import Agent\n",
    "import Config\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Initialize the config in which all hyperparams are stored\n",
    "config = Config.config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_file_name = 'Tennis_Windows_x86_64/Tennis.exe' # Path to the Unity environment.\n",
    "n_episodes = 50000              # Number of episodes to train on\n",
    "n_episodes_to_watch = 5         # Numbers of episodes to watch the trained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up the environment and watch an untrained agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step we start the environment, save the `brain_name`, which we need to interact with the environment and save the size of the state- and action-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_file_name)\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "\n",
    "# dimension of the state-space \n",
    "num_agents = env_info.vector_observations.shape[0]\n",
    "state_size = env_info.vector_observations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison we'll let an agent take random actions and observe the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall best score: 0.0\n"
     ]
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=False)[brain_name]      # reset the environment    \n",
    "states = env_info.vector_observations                   # get the current state (for each agent)\n",
    "max_scores = -np.inf                                    # initialize the max score\n",
    "for _ in range(5):\n",
    "    scores = np.zeros(num_agents)                       # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            max_scores = np.max([max_scores, np.max(scores)]) # Update the max scores\n",
    "            break\n",
    "print(f\"Overall best score: {max_scores:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train an agent\n",
    "\n",
    "In the next code cells we initialize and train the agents. The trained model-parameters for the actor and critic will evaluated every 250 episodes and saved, every time a new best avg. score is achieved, in `checkpoint_xxx.pth`. \n",
    "\n",
    "To proof stability, training will be continued for 1000 more episodes.\n",
    "\n",
    "Training will be done on the gpu if one is available.\n",
    "\n",
    "If you just want to analyse already trained agents, you can skip this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(n_episodes=10000):\n",
    "    \"\"\"Training the agents.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    scores_window.append(0)\n",
    "    \n",
    "    ep_after_solving = 1000\n",
    "    env_solved = False\n",
    "    best_mean_score = 0\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name] # reset the environment\n",
    "        state = env_info.vector_observations\n",
    "        score_1 = 0\n",
    "        score_2 = 0\n",
    "        \n",
    "        # Decay the action-noise and reset the randomer\n",
    "        # The action-noise is linear dependant of the avg. score. \n",
    "        # Score==0: eps=config.EPS_START\n",
    "        # Score==0.3: eps=config.EPS_MIN\n",
    "        new_eps = np.mean(scores_window)*(config.EPSILON_MIN-config.EPSILON_START)/.3 + config.EPSILON_START\n",
    "        agent_1.epsilon = new_eps\n",
    "        agent_2.epsilon = new_eps\n",
    "        agent_1.noise.reset()\n",
    "        agent_2.noise.reset()\n",
    "\n",
    "        while True:\n",
    "            \n",
    "            # Select action\n",
    "            action_1 = agent_1.act(np.expand_dims(state[0], 0))\n",
    "            action_2 = agent_2.act(np.expand_dims(state[1], 0))\n",
    "\n",
    "            action = [action_1, action_2]\n",
    "            env_info = env.step(action)[brain_name]         # send the action to the environment\n",
    "            next_state = env_info.vector_observations       # get the next state\n",
    "            reward = env_info.rewards                       # get the reward\n",
    "            done = env_info.local_done                      # see if episode has finished\n",
    "            \n",
    "            agent_1.step(state[0], action_1[0], reward[0], next_state[0], done[0])\n",
    "            agent_2.step(state[1], action_2[0], reward[1], next_state[1], done[1])\n",
    "          \n",
    "            # Save the next_state as state and add the rewards to the current scores\n",
    "            state = next_state\n",
    "            score_1 += reward[0]\n",
    "            score_2 += reward[1]\n",
    "            if np.any(done): \n",
    "                break \n",
    "\n",
    "                \n",
    "        scores_window.append(np.max([score_1, score_2]))       # save most recent score to the windowed score\n",
    "        scores.append(np.max([score_1, score_2]))              # save most recent score\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tLast Score: {:.2f}'.format(i_episode, np.mean(scores_window), scores[-1]), end=\"\")\n",
    "        if i_episode % 250 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "            \n",
    "            # Save weights if the avg. score is higher than it ever was before\n",
    "            if np.mean(scores_window) > best_mean_score:\n",
    "                \n",
    "                best_mean_score = np.mean(scores_window)\n",
    "                torch.save(agent_1.critic_local.state_dict(), f\"Results/checkpoint_agent_1_critic.pth\")\n",
    "                torch.save(agent_1.actor_local.state_dict(), f\"Results/checkpoint_agent_1_actor.pth\")\n",
    "                torch.save(agent_2.critic_local.state_dict(), f\"Results/checkpoint_agent_2_critic.pth\")\n",
    "                torch.save(agent_2.actor_local.state_dict(), f\"Results/checkpoint_agent_2_actor.pth\")\n",
    "\n",
    "            # Saving Scores\n",
    "            pickle.dump(scores, open(f\"Results/Scores.pkl\", 'wb'))\n",
    "        \n",
    "        # If the environment is solved the first time, save the current episode in env_solved\n",
    "        if np.mean(scores_window)>=.3 and not env_solved:\n",
    "            env_solved = i_episode\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # Check if the after-run is finished and then stop training\n",
    "        if env_solved and (i_episode >= env_solved + ep_after_solving  or (i_episode>=n_episodes+1)):\n",
    "            break\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 250\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 500\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 750\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 1000\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 1250\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 1500\tAverage Score: 0.02\tLast Score: 0.00\n",
      "Episode 1750\tAverage Score: 0.04\tLast Score: 0.10\n",
      "Episode 2000\tAverage Score: 0.04\tLast Score: 0.10\n",
      "Episode 2250\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 2500\tAverage Score: 0.02\tLast Score: 0.09\n",
      "Episode 2750\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 3000\tAverage Score: 0.02\tLast Score: 0.00\n",
      "Episode 3250\tAverage Score: 0.02\tLast Score: 0.10\n",
      "Episode 3500\tAverage Score: 0.02\tLast Score: 0.00\n",
      "Episode 3750\tAverage Score: 0.01\tLast Score: 0.10\n",
      "Episode 4000\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 4250\tAverage Score: 0.02\tLast Score: 0.09\n",
      "Episode 4500\tAverage Score: 0.02\tLast Score: 0.00\n",
      "Episode 4750\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 5000\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 5250\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 5500\tAverage Score: 0.01\tLast Score: 0.00\n",
      "Episode 5750\tAverage Score: 0.02\tLast Score: 0.00\n",
      "Episode 6000\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 6250\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 6500\tAverage Score: 0.03\tLast Score: 0.09\n",
      "Episode 6750\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 7000\tAverage Score: 0.03\tLast Score: 0.00\n",
      "Episode 7250\tAverage Score: 0.04\tLast Score: 0.09\n",
      "Episode 7500\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 7750\tAverage Score: 0.05\tLast Score: 0.09\n",
      "Episode 8000\tAverage Score: 0.05\tLast Score: 0.09\n",
      "Episode 8250\tAverage Score: 0.04\tLast Score: 0.00\n",
      "Episode 8500\tAverage Score: 0.05\tLast Score: 0.09\n",
      "Episode 8750\tAverage Score: 0.04\tLast Score: 0.09\n",
      "Episode 9000\tAverage Score: 0.05\tLast Score: 0.09\n",
      "Episode 9250\tAverage Score: 0.04\tLast Score: 0.00\n",
      "Episode 9500\tAverage Score: 0.05\tLast Score: 0.09\n",
      "Episode 9750\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 10000\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 10250\tAverage Score: 0.03\tLast Score: 0.09\n",
      "Episode 10500\tAverage Score: 0.05\tLast Score: 0.10\n",
      "Episode 10750\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 11000\tAverage Score: 0.04\tLast Score: 0.00\n",
      "Episode 11250\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 11500\tAverage Score: 0.04\tLast Score: 0.10\n",
      "Episode 11750\tAverage Score: 0.05\tLast Score: 0.00\n",
      "Episode 12000\tAverage Score: 0.06\tLast Score: 0.09\n",
      "Episode 12250\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 12500\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 12750\tAverage Score: 0.06\tLast Score: 0.10\n",
      "Episode 13000\tAverage Score: 0.06\tLast Score: 0.10\n",
      "Episode 13250\tAverage Score: 0.07\tLast Score: 0.00\n",
      "Episode 13500\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 13750\tAverage Score: 0.06\tLast Score: 0.10\n",
      "Episode 14000\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 14250\tAverage Score: 0.07\tLast Score: 0.09\n",
      "Episode 14500\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 14750\tAverage Score: 0.06\tLast Score: 0.09\n",
      "Episode 15000\tAverage Score: 0.06\tLast Score: 0.00\n",
      "Episode 15250\tAverage Score: 0.07\tLast Score: 0.09\n",
      "Episode 15500\tAverage Score: 0.08\tLast Score: 0.09\n",
      "Episode 15750\tAverage Score: 0.08\tLast Score: 0.09\n",
      "Episode 16000\tAverage Score: 0.10\tLast Score: 0.09\n",
      "Episode 16250\tAverage Score: 0.07\tLast Score: 0.00\n",
      "Episode 16500\tAverage Score: 0.09\tLast Score: 0.09\n",
      "Episode 16750\tAverage Score: 0.16\tLast Score: 0.80\n",
      "Episode 17000\tAverage Score: 0.17\tLast Score: 0.50\n",
      "Episode 17250\tAverage Score: 0.17\tLast Score: 0.20\n",
      "Episode 17500\tAverage Score: 0.15\tLast Score: 0.10\n",
      "Episode 17750\tAverage Score: 0.14\tLast Score: 0.10\n",
      "Episode 18000\tAverage Score: 0.22\tLast Score: 0.20\n",
      "Episode 18250\tAverage Score: 0.18\tLast Score: 0.10\n",
      "Episode 18500\tAverage Score: 0.19\tLast Score: 0.10\n",
      "Episode 18750\tAverage Score: 0.24\tLast Score: 0.10\n",
      "Episode 18997\tAverage Score: 0.30\tLast Score: 0.30\n",
      "Environment solved in 18997 episodes!\tAverage Score: 0.30\n",
      "Episode 19000\tAverage Score: 0.30\tLast Score: 0.10\n",
      "Episode 19250\tAverage Score: 0.28\tLast Score: 0.20\n",
      "Episode 19500\tAverage Score: 0.33\tLast Score: 0.40\n",
      "Episode 19750\tAverage Score: 0.27\tLast Score: 0.50\n",
      "Episode 19997\tAverage Score: 0.27\tLast Score: 0.10"
     ]
    }
   ],
   "source": [
    "# Create the Agent and let it train\n",
    "agent_1 = Agent(config, state_size, action_size, 1, seed=0)\n",
    "agent_2 = Agent(config, state_size, action_size, 1, seed=1234)\n",
    "\n",
    "# Train the agent and save the scores for later analysis\n",
    "scores = train_agent(n_episodes=n_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Whatch trained agents\n",
    "Next we can watch a trained agent play tennis for `n_episodes_to_watch` episodes. Therefore we load the model-parameters, reset the environment and loop through the specified number of episodes only using the calculated actions without any random noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_1 = Agent(config, state_size, action_size, 1, seed=0)\n",
    "agent_2 = Agent(config, state_size, action_size, 1, seed=1234)\n",
    "\n",
    "agent_1.actor_local.load_state_dict(torch.load(\"Results/checkpoint_agent_1_actor.pth\", map_location=agent_1.config.device))\n",
    "agent_1.critic_local.load_state_dict(torch.load(\"Results/checkpoint_agent_1_critic.pth\", map_location=agent_1.config.device))\n",
    "agent_2.actor_local.load_state_dict(torch.load(\"Results/checkpoint_agent_2_actor.pth\", map_location=agent_2.config.device))\n",
    "agent_2.critic_local.load_state_dict(torch.load(\"Results/checkpoint_agent_2_critic.pth\", map_location=agent_2.config.device))\n",
    "\n",
    "agent_1.is_training = False\n",
    "agent_2.is_training = False\n",
    "\n",
    "scores = []                            # list containing scores from each episode\n",
    "best_score = -np.inf                   # initialize the best score\n",
    "for i_episode in range(1, n_episodes_to_watch+1):    # Loop through five episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "    state = env_info.vector_observations\n",
    "    score_1 = 0\n",
    "    score_2 = 0\n",
    "    \n",
    "    while True:\n",
    "        action_1 = agent_1.act(np.expand_dims(state[0], 0))\n",
    "        action_2 = agent_2.act(np.expand_dims(state[1], 0))\n",
    "\n",
    "        action = [action_1, action_2]\n",
    "        env_info = env.step(action)[brain_name]         # send the action to the environment\n",
    "        \n",
    "        state = env_info.vector_observations            # get the new state\n",
    "        reward = env_info.rewards                       # get the reward\n",
    "        done = env_info.local_done                      # see if episode has finished\n",
    "        \n",
    "        score_1 += reward[0]\n",
    "        score_2 += reward[1]\n",
    "        \n",
    "        if np.any(done):\n",
    "            break \n",
    "    temp_score = np.max([score_1, score_2])\n",
    "    if temp_score > best_score:\n",
    "        best_score = temp_score\n",
    "    scores.append(temp_score)                           # save most recent score\n",
    "\n",
    "print(f\"Avg. score: {np.mean(scores):.3}, Max score: {best_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Plot the learning-curve\n",
    "In the last section we want to take a look at the learning curve. For orientation the goal-score (30) is also plottet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Score')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_scores = pickle.load(open(f\"Results/Scores.pkl\", 'rb'))\n",
    "scores_ma = pd.DataFrame(training_scores).rolling(window=100).mean()\n",
    "plt.plot(range(1, len(training_scores)+1), training_scores, 'b')\n",
    "plt.plot(scores_ma.index.values, scores_ma.values, 'r')\n",
    "plt.plot(np.ones(len(training_scores))*.3, 'k--')\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
